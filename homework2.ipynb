{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebf211b-dbaf-4844-9593-e69a6cac69f9",
   "metadata": {},
   "source": [
    "# [요구사항 1] titanic 딥러닝 모델 기본 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T02:43:31.925775Z",
     "start_time": "2025-10-17T02:43:27.990951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: C:\\Users\\hanle\\git\\link_dl\n",
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\hanle\\git\\link_dl\\_04_your_code\\wandb\\run-20251017_131253-q54nntmr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/q54nntmr' target=\"_blank\">2025-10-17_13-12-53</a></strong> to <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/q54nntmr' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/q54nntmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanle\\git\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\hanle\\git\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:147: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Embarked', 'title', 'family_num', 'alone'],\n",
      "      dtype='object')\n",
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
      "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
      "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
      "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
      "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
      "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
      "5       0.0       3    1  29.0      0      0   8.4583         1      2   \n",
      "6       0.0       1    1  54.0      0      0  51.8625         2      2   \n",
      "7       0.0       3    1   2.0      3      1  21.0750         2      0   \n",
      "8       1.0       3    0  27.0      0      2  11.1333         2      3   \n",
      "9       1.0       2    0  14.0      1      0  30.0708         0      3   \n",
      "\n",
      "   family_num  alone  \n",
      "0           1    0.0  \n",
      "1           1    0.0  \n",
      "2           0    1.0  \n",
      "3           1    0.0  \n",
      "4           0    1.0  \n",
      "5           0    1.0  \n",
      "6           0    1.0  \n",
      "7           4    0.0  \n",
      "8           2    0.0  \n",
      "9           1    0.0  \n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
      "Input dim: 10\n",
      "################################################## 1\n",
      "[Epoch    1] T_loss: 0.9574 | V_loss: 0.7444\n",
      "[Epoch   50] T_loss: 0.5689 | V_loss: 0.6235\n",
      "[Epoch  100] T_loss: 0.5706 | V_loss: 0.6212\n",
      "[Epoch  150] T_loss: 0.5678 | V_loss: 0.6181\n",
      "[Epoch  200] T_loss: 0.5608 | V_loss: 0.6161\n",
      "[Epoch  250] T_loss: 0.5585 | V_loss: 0.6143\n",
      "[Epoch  300] T_loss: 0.5526 | V_loss: 0.6126\n",
      "WandB run URL: https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/q54nntmr\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>Training loss</td><td>██▇▇▆▇▇▆▇▅▄▅▅▆▅▃▃▄▃▅▄▄▄▃▄▂▁▂▂▂▃▁▃▂▂▂▃▂▁▁</td></tr><tr><td>Validation loss</td><td>▇▇▇███▇▇▇▇▆▇▆▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▄▂▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>300</td></tr><tr><td>Training loss</td><td>0.55258</td></tr><tr><td>Validation loss</td><td>0.61259</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-17_13-12-53</strong> at: <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/q54nntmr' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/q54nntmr</a><br> View project at: <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_131253-q54nntmr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- JUPYTER NOTEBOOK용 설정 ---\n",
    "import os, sys\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "\n",
    "# repo 루트 자동 탐색: _01_code 폴더가 보이는 위치를 찾음\n",
    "repo_root = None\n",
    "cur = Path.cwd()\n",
    "for p in [cur, *cur.parents]:\n",
    "    if (p / \"_01_code\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "if repo_root is None:\n",
    "    # repo_root = Path(r\"C:\\Users\\hanle\\git\\link_dl\")\n",
    "    raise RuntimeError(\"repo_root 탐색 실패함\")\n",
    "\n",
    "BASE_PATH = str(repo_root.resolve())\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "if BASE_PATH not in sys.path:\n",
    "    sys.path.append(BASE_PATH)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "# Titanic 데이터셋 \n",
    "from _03_homeworks.homework_2.titanic_dataset import get_preprocessed_dataset\n",
    "\n",
    "\n",
    "# Jupyter Notebook 환경에서 argparse를 대신\n",
    "args = SimpleNamespace(\n",
    "    wandb=True,           # WandB 켜기/끄기 (True/False)\n",
    "    batch_size=128,       # Batch size 바꿔가면서 실행 (16,32,64,128)\n",
    "    epochs=300,           # Epochs\n",
    "    learning_rate=1e-3,   # LR\n",
    "    n_hidden_unit_list=[64, 32],  # 은닉층\n",
    "    activation=\"leakyrelu\"     # Activation Function 바꿔가면서 실행 (relu, elu , leakyrelu, sigmoid)\n",
    ")\n",
    "\n",
    "# 활성화 함수 미리 정의\n",
    "def get_activation(name: str) -> nn.Module:\n",
    "    name = name.lower()\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    if name == \"elu\":\n",
    "        return nn.ELU(alpha=1.0)\n",
    "    if name in \"leakyrelu\":\n",
    "        return nn.LeakyReLU(negative_slope=0.01)\n",
    "    if name == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    raise ValueError(f\"Unsupported activation: {name}\")\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    #california_housing_dataset = CaliforniaHousingDataset()\n",
    "    #print(california_housing_dataset)\n",
    "\n",
    "    #train_dataset, validation_dataset = random_split(california_housing_dataset, [0.8, 0.2])\n",
    "    #print(len(train_dataset), len(validation_dataset))\n",
    "    \"\"\"\n",
    "    titanic_dataset.py에서 dataset을 받아 DataLoader 구성\n",
    "    \"\"\"\n",
    "    train_dataset, validation_dataset, _ = get_preprocessed_dataset()\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=validation_dataset,\n",
    "        batch_size=len(validation_dataset)  \n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        hidden1, hidden2 = args.n_hidden_unit_list\n",
    "        act = get_activation(args.activation)\n",
    "        # 동일 종류 activation 새 인스턴스를 두 번 쓰기\n",
    "        def new_act_like(a: nn.Module) -> nn.Module:\n",
    "            if isinstance(a, nn.ELU):\n",
    "                return nn.ELU(alpha=a.alpha)\n",
    "            if isinstance(a, nn.LeakyReLU):\n",
    "                return nn.LeakyReLU(negative_slope=a.negative_slope)\n",
    "            return a.__class__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, hidden1),\n",
    "            act,\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            new_act_like(act),\n",
    "            nn.Linear(hidden2, n_output),  # logits (2 클래스)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def get_model_and_optimizer(input_dim):\n",
    "    \"\"\"\n",
    "    Titanic: input_dim=10, output_dim=2 (이진분류)\n",
    "    \"\"\"\n",
    "    my_model = MyModel(n_input=input_dim, n_output=2)\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=args.learning_rate)\n",
    "    return my_model, optimizer\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, device):\n",
    "    n_epochs = args.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()  \n",
    "    next_print_epoch = 50\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "\n",
    "        for batch in train_data_loader:\n",
    "            # titanic_dataset은 dict 배치 {'input': Tensor, 'target': Tensor}\n",
    "            input_t = batch['input'].to(device)\n",
    "            target_t = batch['target'].to(device)  # LongTensor(0/1)\n",
    "\n",
    "            logits = model(input_t)\n",
    "            loss = loss_fn(logits, target_t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        loss_validation = 0.0\n",
    "        num_validations = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                input_v = batch['input'].to(device)\n",
    "                target_v = batch['target'].to(device)\n",
    "\n",
    "                logits_v = model(input_v)\n",
    "                loss_v = loss_fn(logits_v, target_v)\n",
    "\n",
    "                loss_validation += loss_v.item()\n",
    "                num_validations += 1\n",
    "\n",
    "        # wandb 로깅\n",
    "        if args.wandb:\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training loss\": loss_train / max(1, num_trains),\n",
    "                \"Validation loss\": loss_validation / max(1, num_validations)\n",
    "            })\n",
    "\n",
    "        if epoch % next_print_epoch == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"[Epoch {epoch:>4}] \"\n",
    "                f\"T_loss: {loss_train / max(1, num_trains):.4f} | \"\n",
    "                f\"V_loss: {loss_validation / max(1, num_validations):.4f}\"\n",
    "            )\n",
    "\n",
    "# 실행: 노트북에서 함수처럼 호출\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if args.wandb:\n",
    "    wandb.init(\n",
    "        mode=\"online\",\n",
    "        project=\"titanic_training\",\n",
    "        notes=\"Titanic binary classification with FCN (Notebook)\",\n",
    "        tags=[\"titanic\", \"binary_classification\", \"fcn\", args.activation, f\"bs{args.batch_size}\"],\n",
    "        name=datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "        config=dict(\n",
    "            epochs=args.epochs,\n",
    "            batch_size=args.batch_size,\n",
    "            learning_rate=args.learning_rate,\n",
    "            n_hidden_unit_list=args.n_hidden_unit_list,\n",
    "            activation=args.activation\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_data_loader, validation_data_loader = get_data()\n",
    "\n",
    "# 입력 차원 자동 추론 (첫 배치 한 번 꺼내보기)\n",
    "sample_batch = next(iter(train_data_loader))\n",
    "input_dim = sample_batch['input'].shape[1]\n",
    "print(\"Input dim:\", input_dim)\n",
    "\n",
    "model, optimizer = get_model_and_optimizer(input_dim)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "training_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "if args.wandb:\n",
    "    print(\"WandB run URL:\", wandb.run.url)\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971677b-0e61-49fe-9f0c-60558048e9cc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# [요구사항 2] Activation\tFunction 과 Batch Size 변경 및 선택하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c51b99-e78b-4f2e-bea0-b130ea7629c5",
   "metadata": {},
   "source": [
    "# ----- 학습 결과 [Activaton Function - Batch Size] -----\n",
    "### Epoch: 300으로 통일\n",
    "\n",
    "[Sigmoid-16]\n",
    "[Epoch    1] T_loss: 0.7580 | V_loss: 0.7615\n",
    "[Epoch   50] T_loss: 0.6609 | V_loss: 0.6392\n",
    "[Epoch  100] T_loss: 0.6463 | V_loss: 0.6271\n",
    "[Epoch  150] T_loss: 0.6322 | V_loss: 0.6163\n",
    "[Epoch  200] T_loss: 0.6161 | V_loss: 0.6072\n",
    "[Epoch  250] T_loss: 0.6095 | V_loss: 0.6028\n",
    "[Epoch  300] T_loss: 0.6033 | V_loss: 0.6009\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.60332\n",
    "Validation loss\t0.60094\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Sigmoid-32]\n",
    "[Epoch    1] T_loss: 0.7196 | V_loss: 0.7194\n",
    "[Epoch   50] T_loss: 0.6621 | V_loss: 0.6485\n",
    "[Epoch  100] T_loss: 0.6548 | V_loss: 0.6429\n",
    "[Epoch  150] T_loss: 0.6531 | V_loss: 0.6373\n",
    "[Epoch  200] T_loss: 0.6460 | V_loss: 0.6315\n",
    "[Epoch  250] T_loss: 0.6338 | V_loss: 0.6256\n",
    "[Epoch  300] T_loss: 0.6311 | V_loss: 0.6200\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.63114\n",
    "Validation loss\t0.62003\n",
    "\n",
    "\n",
    "\n",
    "[Sigmoid-64]\n",
    "[Epoch    1] T_loss: 0.6734 | V_loss: 0.6489\n",
    "[Epoch   50] T_loss: 0.6643 | V_loss: 0.6491\n",
    "[Epoch  100] T_loss: 0.6658 | V_loss: 0.6474\n",
    "[Epoch  150] T_loss: 0.6599 | V_loss: 0.6452\n",
    "[Epoch  200] T_loss: 0.6624 | V_loss: 0.6427\n",
    "[Epoch  250] T_loss: 0.6613 | V_loss: 0.6412\n",
    "[Epoch  300] T_loss: 0.6547 | V_loss: 0.6381\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.65468\n",
    "Validation loss\t0.63808\n",
    "\n",
    "\n",
    "\n",
    "[Sigmoid-128]\n",
    "[Epoch    1] T_loss: 0.7839 | V_loss: 0.8069\n",
    "[Epoch   50] T_loss: 0.6806 | V_loss: 0.6742\n",
    "[Epoch  100] T_loss: 0.6740 | V_loss: 0.6578\n",
    "[Epoch  150] T_loss: 0.6721 | V_loss: 0.6532\n",
    "[Epoch  200] T_loss: 0.6690 | V_loss: 0.6505\n",
    "[Epoch  250] T_loss: 0.6698 | V_loss: 0.6485\n",
    "[Epoch  300] T_loss: 0.6680 | V_loss: 0.6465\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.66796\n",
    "Validation loss\t0.64655\n",
    "\n",
    "\n",
    "\n",
    "[ReLu-16]\n",
    "[Epoch    1] T_loss: 0.6445 | V_loss: 0.6536\n",
    "[Epoch   50] T_loss: 0.5833 | V_loss: 0.5949\n",
    "[Epoch  100] T_loss: 0.5718 | V_loss: 0.5982\n",
    "[Epoch  150] T_loss: 0.5552 | V_loss: 0.6144\n",
    "[Epoch  200] T_loss: 0.5562 | V_loss: 0.5813\n",
    "[Epoch  250] T_loss: 0.5224 | V_loss: 0.6294\n",
    "[Epoch  300] T_loss: 0.5034 | V_loss: 0.5736\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.50345\n",
    "Validation loss\t0.5736\n",
    "\n",
    "\n",
    "\n",
    "[ReLu-32]\n",
    "[Epoch    1] T_loss: 0.7099 | V_loss: 0.6487\n",
    "[Epoch   50] T_loss: 0.5776 | V_loss: 0.5949\n",
    "[Epoch  100] T_loss: 0.5745 | V_loss: 0.6041\n",
    "[Epoch  150] T_loss: 0.5678 | V_loss: 0.5765\n",
    "[Epoch  200] T_loss: 0.5626 | V_loss: 0.6033\n",
    "[Epoch  250] T_loss: 0.5547 | V_loss: 0.5981\n",
    "[Epoch  300] T_loss: 0.5368 | V_loss: 0.5751\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.53681\n",
    "Validation loss\t0.57507\n",
    "\n",
    "\n",
    "\n",
    "[ReLu-64]\n",
    "[Epoch    1] T_loss: 0.6898 | V_loss: 0.6486\n",
    "[Epoch   50] T_loss: 0.5876 | V_loss: 0.6235\n",
    "[Epoch  100] T_loss: 0.5795 | V_loss: 0.6249\n",
    "[Epoch  150] T_loss: 0.5459 | V_loss: 0.6226\n",
    "[Epoch  200] T_loss: 0.5426 | V_loss: 0.6350\n",
    "[Epoch  250] T_loss: 0.5469 | V_loss: 0.6585\n",
    "[Epoch  300] T_loss: 0.5487 | V_loss: 0.6117\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.54869\n",
    "Validation loss\t0.61165\n",
    "\n",
    "\n",
    "\n",
    "[ReLu-128]\n",
    "[Epoch    1] T_loss: 0.6663 | V_loss: 0.6368\n",
    "[Epoch   50] T_loss: 0.6018 | V_loss: 0.5987\n",
    "[Epoch  100] T_loss: 0.5934 | V_loss: 0.5978\n",
    "[Epoch  150] T_loss: 0.5863 | V_loss: 0.5896\n",
    "[Epoch  200] T_loss: 0.5846 | V_loss: 0.5903\n",
    "[Epoch  250] T_loss: 0.5778 | V_loss: 0.5825\n",
    "[Epoch  300] T_loss: 0.5860 | V_loss: 0.5822\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.58595\n",
    "Validation loss\t0.58221\n",
    "\n",
    "\n",
    "\n",
    "[ELU-16]\n",
    "[Epoch    1] T_loss: 0.7640 | V_loss: 0.6499\n",
    "[Epoch   50] T_loss: 0.5656 | V_loss: 0.6225\n",
    "[Epoch  100] T_loss: 0.5388 | V_loss: 0.5857\n",
    "[Epoch  150] T_loss: 0.5156 | V_loss: 0.5381\n",
    "[Epoch  200] T_loss: 0.4975 | V_loss: 0.5414\n",
    "[Epoch  250] T_loss: 0.4666 | V_loss: 0.4924\n",
    "[Epoch  300] T_loss: 0.4728 | V_loss: 0.5361\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.47278\n",
    "Validation loss\t0.53614\n",
    "\n",
    "\n",
    "[ELU-32]\n",
    "[Epoch    1] T_loss: 0.6666 | V_loss: 0.6611\n",
    "[Epoch   50] T_loss: 0.5651 | V_loss: 0.6164\n",
    "[Epoch  100] T_loss: 0.5557 | V_loss: 0.6267\n",
    "[Epoch  150] T_loss: 0.5421 | V_loss: 0.6424\n",
    "[Epoch  200] T_loss: 0.5424 | V_loss: 0.6004\n",
    "[Epoch  250] T_loss: 0.5182 | V_loss: 0.6156\n",
    "[Epoch  300] T_loss: 0.5171 | V_loss: 0.5810\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.51711\n",
    "Validation loss\t0.58097\n",
    "\n",
    "\n",
    "[ELU-64]\n",
    "[Epoch    1] T_loss: 0.7334 | V_loss: 0.6572\n",
    "[Epoch   50] T_loss: 0.5987 | V_loss: 0.5751\n",
    "[Epoch  100] T_loss: 0.5734 | V_loss: 0.5602\n",
    "[Epoch  150] T_loss: 0.5843 | V_loss: 0.5510\n",
    "[Epoch  200] T_loss: 0.5513 | V_loss: 0.5422\n",
    "[Epoch  250] T_loss: 0.5459 | V_loss: 0.5337\n",
    "[Epoch  300] T_loss: 0.5627 | V_loss: 0.5317\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.56273\n",
    "Validation loss\t0.53165\n",
    "\n",
    "\n",
    "[ELU-128]\n",
    "[Epoch    1] T_loss: 0.7228 | V_loss: 0.6568\n",
    "[Epoch   50] T_loss: 0.6003 | V_loss: 0.6150\n",
    "[Epoch  100] T_loss: 0.5967 | V_loss: 0.6157\n",
    "[Epoch  150] T_loss: 0.5919 | V_loss: 0.6130\n",
    "[Epoch  200] T_loss: 0.5847 | V_loss: 0.6103\n",
    "[Epoch  250] T_loss: 0.5882 | V_loss: 0.6058\n",
    "[Epoch  300] T_loss: 0.5795 | V_loss: 0.6011\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.57952\n",
    "Validation loss\t0.60114\n",
    "\n",
    "\n",
    "\n",
    "[LeakyReLU-16]\n",
    "[Epoch    1] T_loss: 0.6221 | V_loss: 0.5429\n",
    "[Epoch   50] T_loss: 0.5857 | V_loss: 0.5268\n",
    "[Epoch  100] T_loss: 0.5728 | V_loss: 0.5194\n",
    "[Epoch  150] T_loss: 0.5555 | V_loss: 0.4965\n",
    "[Epoch  200] T_loss: 0.5354 | V_loss: 0.4887\n",
    "[Epoch  250] T_loss: 0.5259 | V_loss: 0.4804\n",
    "[Epoch  300] T_loss: 0.5011 | V_loss: 0.4949\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.50107\n",
    "Validation loss\t0.49494\n",
    "\n",
    "\n",
    "\n",
    "[LeakyReLU-32]\n",
    "[Epoch    1] T_loss: 0.7664 | V_loss: 0.6549\n",
    "[Epoch   50] T_loss: 0.5957 | V_loss: 0.5654\n",
    "[Epoch  100] T_loss: 0.5793 | V_loss: 0.5505\n",
    "[Epoch  150] T_loss: 0.5789 | V_loss: 0.5397\n",
    "[Epoch  200] T_loss: 0.5642 | V_loss: 0.5274\n",
    "[Epoch  250] T_loss: 0.5557 | V_loss: 0.5145\n",
    "[Epoch  300] T_loss: 0.5448 | V_loss: 0.5078\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.54477\n",
    "Validation loss\t0.50777\n",
    "\n",
    "\n",
    "[LeakyReLU-64]\n",
    "[Epoch    1] T_loss: 0.7183 | V_loss: 0.6379\n",
    "[Epoch   50] T_loss: 0.5979 | V_loss: 0.5763\n",
    "[Epoch  100] T_loss: 0.5943 | V_loss: 0.5859\n",
    "[Epoch  150] T_loss: 0.5818 | V_loss: 0.5736\n",
    "[Epoch  200] T_loss: 0.5684 | V_loss: 0.5744\n",
    "[Epoch  250] T_loss: 0.5576 | V_loss: 0.5667\n",
    "[Epoch  300] T_loss: 0.5524 | V_loss: 0.5630\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.55242\n",
    "Validation loss\t0.56301\n",
    "\n",
    "\n",
    "[LeakyReLU-128]\n",
    "[Epoch    1] T_loss: 0.9574 | V_loss: 0.7444\n",
    "[Epoch   50] T_loss: 0.5689 | V_loss: 0.6235\n",
    "[Epoch  100] T_loss: 0.5706 | V_loss: 0.6212\n",
    "[Epoch  150] T_loss: 0.5678 | V_loss: 0.6181\n",
    "[Epoch  200] T_loss: 0.5608 | V_loss: 0.6161\n",
    "[Epoch  250] T_loss: 0.5585 | V_loss: 0.6143\n",
    "[Epoch  300] T_loss: 0.5526 | V_loss: 0.6126\n",
    "\n",
    "Run summary:\n",
    "Epoch\t300\n",
    "Training loss\t0.55258\n",
    "Validation loss\t0.61259\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c41e8-6574-435e-af03-450c0f93be0d",
   "metadata": {},
   "source": [
    "## 결론\n",
    "활성화 함수별 성능 순위:\n",
    "ELU >= LeakyReLU > ReLU > Sigmoid\n",
    "\n",
    "Batch Size별 경향:\n",
    "- 배치 사이즈 16, 32는 빠르게 학습하고 Validation loss가 낮음\n",
    "- 배치 사이즈 64, 128는 안정적이지만 수렴 속도가 느리고 loss가 약간 높음\n",
    "\n",
    "최종적으로 가장 좋은 조합:\n",
    "- ELU + Batch Size 64 (Validation loss = 0.5317)\n",
    "-> 수렴 안정, 손실 가장 낮음, 과적합 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca0876-42e7-4463-8186-160ac1dcdc06",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "# [요구사항 3] 테스트 및 submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff361ea8-427f-4930-a385-7c7c3071555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결론으로 이어지는 간소화 버전 training_loop: best_state/best_epoch/best_val 반환\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, device):\n",
    "    n_epochs = args.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    next_print_epoch = 50\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    best_val = None\n",
    "    best_epoch = None\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        loss_train, num_trains = 0.0, 0\n",
    "        for batch in train_data_loader:\n",
    "            x = batch[\"input\"].to(device)\n",
    "            y = batch[\"target\"].to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        loss_val, num_vals = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                x = batch[\"input\"].to(device)\n",
    "                y = batch[\"target\"].to(device)\n",
    "                logits = model(x)\n",
    "                loss = loss_fn(logits, y)\n",
    "                loss_val += loss.item()\n",
    "                num_vals += 1\n",
    "\n",
    "        tr = loss_train / max(1, num_trains)\n",
    "        vl = loss_val / max(1, num_vals)\n",
    "\n",
    "        # best 갱신 기록\n",
    "        if (best_val is None) or (vl < best_val - 1e-12):\n",
    "            best_val = vl\n",
    "            best_epoch = epoch\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if args.wandb:\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training loss\": tr,\n",
    "                \"Validation loss\": vl,\n",
    "                \"Best val loss\": best_val,\n",
    "                \"Best val epoch\": best_epoch\n",
    "            })\n",
    "\n",
    "        if epoch % next_print_epoch == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:>4}] T_loss: {tr:.4f} | V_loss: {vl:.4f} | best@{best_epoch}={best_val:.4f}\")\n",
    "\n",
    "    return best_state, best_epoch, best_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad176f41-4391-4fdb-b5f0-d9b45cc52e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanle\\git\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\hanle\\git\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:147: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Embarked', 'title', 'family_num', 'alone'],\n",
      "      dtype='object')\n",
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
      "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
      "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
      "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
      "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
      "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
      "5       0.0       3    1  29.0      0      0   8.4583         1      2   \n",
      "6       0.0       1    1  54.0      0      0  51.8625         2      2   \n",
      "7       0.0       3    1   2.0      3      1  21.0750         2      0   \n",
      "8       1.0       3    0  27.0      0      2  11.1333         2      3   \n",
      "9       1.0       2    0  14.0      1      0  30.0708         0      3   \n",
      "\n",
      "   family_num  alone  \n",
      "0           1    0.0  \n",
      "1           1    0.0  \n",
      "2           0    1.0  \n",
      "3           1    0.0  \n",
      "4           0    1.0  \n",
      "5           0    1.0  \n",
      "6           0    1.0  \n",
      "7           4    0.0  \n",
      "8           2    0.0  \n",
      "9           1    0.0  \n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\hanle\\git\\link_dl\\_04_your_code\\wandb\\run-20251017_143524-w08585uq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/w08585uq' target=\"_blank\">2025-10-17_14-35-24</a></strong> to <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/w08585uq' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/w08585uq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch    1] T_loss: 0.7429 | V_loss: 0.6679 | best@1=0.6679\n",
      "[Epoch   50] T_loss: 0.5739 | V_loss: 0.5878 | best@49=0.5870\n",
      "[Epoch  100] T_loss: 0.5622 | V_loss: 0.5865 | best@99=0.5777\n",
      "[Epoch  150] T_loss: 0.5619 | V_loss: 0.5708 | best@150=0.5708\n",
      "[Epoch  200] T_loss: 0.5507 | V_loss: 0.5667 | best@198=0.5642\n",
      "[Epoch  250] T_loss: 0.5436 | V_loss: 0.5586 | best@250=0.5586\n",
      "[Epoch  300] T_loss: 0.5221 | V_loss: 0.5543 | best@299=0.5522\n",
      "[Best] epoch=299, val_loss=0.5522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanle\\git\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\hanle\\git\\link_dl\\_03_homeworks\\homework_2\\titanic_dataset.py:147: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Embarked', 'title', 'family_num', 'alone'],\n",
      "      dtype='object')\n",
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
      "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
      "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
      "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
      "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
      "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
      "5       0.0       3    1  29.0      0      0   8.4583         1      2   \n",
      "6       0.0       1    1  54.0      0      0  51.8625         2      2   \n",
      "7       0.0       3    1   2.0      3      1  21.0750         2      0   \n",
      "8       1.0       3    0  27.0      0      2  11.1333         2      3   \n",
      "9       1.0       2    0  14.0      1      0  30.0708         0      3   \n",
      "\n",
      "   family_num  alone  \n",
      "0           1    0.0  \n",
      "1           1    0.0  \n",
      "2           0    1.0  \n",
      "3           1    0.0  \n",
      "4           0    1.0  \n",
      "5           0    1.0  \n",
      "6           0    1.0  \n",
      "7           4    0.0  \n",
      "8           2    0.0  \n",
      "9           1    0.0  \n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
      "Saved submission: C:\\Users\\hanle\\git\\link_dl\\_03_homeworks\\homework_2\\submission.csv\n",
      "WandB run URL: https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/w08585uq\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Best val epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>Best val loss</td><td>█▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>Epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>Training loss</td><td>█▇███▇▆▆▅▆▆▃▄▅▅▅█▆▄▃▁▇▅▅▄▄▃▃▅▃▅▄▄▃▃▁▂▁▄▁</td></tr><tr><td>Validation loss</td><td>▆▆▆▅▅▅▅▆▅▄▄▄▄▅▄▃█▄▃▃▃▂▃▃▄▃▃▂▂▅▂▂▂▂▃▁▂▁▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best val epoch</td><td>299</td></tr><tr><td>Best val loss</td><td>0.55224</td></tr><tr><td>Epoch</td><td>300</td></tr><tr><td>Training loss</td><td>0.52214</td></tr><tr><td>Validation loss</td><td>0.55431</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-17_14-35-24</strong> at: <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/w08585uq' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training/runs/w08585uq</a><br> View project at: <a href='https://wandb.ai/hanleo0714-koreatech/titanic_training' target=\"_blank\">https://wandb.ai/hanleo0714-koreatech/titanic_training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_143524-w08585uq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# 실험 조합 고정 (요구사항 2 결과 반영)\n",
    "args.activation = \"elu\"\n",
    "args.batch_size = 64\n",
    "\n",
    "# 데이터/모델 준비는 기존대로\n",
    "train_data_loader, validation_data_loader = get_data()\n",
    "sample_batch = next(iter(train_data_loader))\n",
    "input_dim = sample_batch['input'].shape[1]\n",
    "model, optimizer = get_model_and_optimizer(input_dim)\n",
    "\n",
    "if args.wandb:\n",
    "    wandb.init(\n",
    "        mode=\"online\",\n",
    "        project=\"titanic_training\",\n",
    "        notes=\"Req3: Best-epoch checkpointing + submission\",\n",
    "        tags=[\"titanic\", \"ELU\", \"bs64\", \"submission\"],\n",
    "        name=datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "        config=dict(\n",
    "            epochs=args.epochs,\n",
    "            batch_size=args.batch_size,\n",
    "            learning_rate=args.learning_rate,\n",
    "            n_hidden_unit_list=args.n_hidden_unit_list,\n",
    "            activation=args.activation\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 학습 + 베스트 체크포인트 획득\n",
    "best_state, best_epoch, best_val = training_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader,\n",
    "    device=device\n",
    ")\n",
    "print(f\"[Best] epoch={best_epoch}, val_loss={best_val:.4f}\")\n",
    "\n",
    "# 베스트 가중치 \n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# test_dataset 로드\n",
    "_, _, test_dataset = get_preprocessed_dataset()\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "# 예측 (logits -> argmax -> 0/1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        logits = model(batch['input'].to(device))\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()  # 0/1\n",
    "        break\n",
    "\n",
    "# PassengerId 가져와서 submission.csv 만들기\n",
    "test_csv_path = Path(BASE_PATH) / \"_03_homeworks\" / \"homework_2\" / \"test.csv\"\n",
    "passenger_ids = pd.read_csv(test_csv_path)[\"PassengerId\"].values\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": passenger_ids,\n",
    "    \"Survived\": preds.astype(int)\n",
    "})\n",
    "\n",
    "save_path = Path(BASE_PATH) / \"_03_homeworks\" / \"homework_2\" / \"submission.csv\"\n",
    "submission.to_csv(save_path, index=False)\n",
    "print(f\"Saved submission: {save_path}\")\n",
    "if args.wandb:\n",
    "    print(\"WandB run URL:\", wandb.run.url)\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53880a9b-02a2-4c70-9e06-cbc0742a5dc5",
   "metadata": {},
   "source": [
    "### 고찰\n",
    "단순히 마지막 Epoch이 아니라,\n",
    "Validation loss가 최소가 되는 Epoch의 모델이 점수가 잘 나올 가능성이 크다.\n",
    "그래서 학습 중에 최저 Validation loss를 계속 추적하도록 코딩했고 갱신될 때마다 그때의 가중치(state_dict) 를 deepcopy로 저장하도록 training_loop를 수정하였다.\n",
    "학습이 끝나면 (best_state, best_epoch, best_val)을 반환해서 베스트 Epoch 가중치로 예측을 진행하였다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc8fa3-8c26-4009-823c-73241f5160c4",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "# [요구사항 4] submission.csv 제출 및 등수확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634d89a-2430-4458-aa84-db98e638982e",
   "metadata": {},
   "source": [
    "![Kaggle Leaderboard](04871484-ce74-4e12-a93a-f62199efd576.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3290b2-8143-48e8-a6df-154f14223a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
